{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Load dataset.\n",
    "dftrain = pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/train.csv')\n",
    "dfeval = pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/eval.csv')\n",
    "y_train = dftrain.pop('survived')\n",
    "y_eval = dfeval.pop('survived')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.random.set_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "for gpu in gpus:\n",
    "    print(\"Name:\", gpu.name, \"  Type:\", gpu.device_type)\n",
    "\n",
    "tf.config.experimental.set_virtual_device_configuration(gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=2048)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc = tf.feature_column\n",
    "CATEGORICAL_COLUMNS = ['sex', 'n_siblings_spouses', 'parch', 'class', 'deck',\n",
    "                       'embark_town', 'alone']\n",
    "NUMERIC_COLUMNS = ['age', 'fare']\n",
    "\n",
    "def one_hot_cat_column(feature_name, vocab):\n",
    "  return fc.indicator_column(\n",
    "      fc.categorical_column_with_vocabulary_list(feature_name,\n",
    "                                                 vocab))\n",
    "feature_columns = []\n",
    "for feature_name in CATEGORICAL_COLUMNS:\n",
    "  # Need to one-hot encode categorical features.\n",
    "  vocabulary = dftrain[feature_name].unique()\n",
    "  feature_columns.append(one_hot_cat_column(feature_name, vocabulary))\n",
    "\n",
    "for feature_name in NUMERIC_COLUMNS:\n",
    "  feature_columns.append(fc.numeric_column(feature_name,\n",
    "                                           dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use entire batch since this is such a small dataset.\n",
    "NUM_EXAMPLES = len(y_train)\n",
    "\n",
    "def make_input_fn(X, y, n_epochs=None, shuffle=True):\n",
    "  def input_fn():\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((X.to_dict(orient='list'), y))\n",
    "    if shuffle:\n",
    "      dataset = dataset.shuffle(NUM_EXAMPLES)\n",
    "    # For training, cycle thru dataset as many times as need (n_epochs=None).\n",
    "    dataset = (dataset\n",
    "      .repeat(n_epochs)\n",
    "      .batch(NUM_EXAMPLES))\n",
    "    return dataset\n",
    "  return input_fn\n",
    "\n",
    "# Training and evaluation input functions.\n",
    "train_input_fn = make_input_fn(dftrain, y_train)\n",
    "eval_input_fn = make_input_fn(dfeval, y_eval, shuffle=False, n_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "  'n_trees': 50,\n",
    "  'max_depth': 3,\n",
    "  'n_batches_per_layer': 1,\n",
    "  # You must enable center_bias = True to get DFCs. This will force the model to\n",
    "  # make an initial prediction before using any features (e.g. use the mean of\n",
    "  # the training labels for regression or log odds for classification when\n",
    "  # using cross entropy loss).\n",
    "  'center_bias': True\n",
    "}\n",
    "\n",
    "est = tf.estimator.BoostedTreesClassifier(feature_columns, **params)\n",
    "# Train model.\n",
    "est.train(train_input_fn, max_steps=100)\n",
    "\n",
    "# Evaluation.\n",
    "results = est.evaluate(eval_input_fn)\n",
    "clear_output()\n",
    "pd.Series(results).to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_memory_params = dict(params)\n",
    "in_memory_params['n_batches_per_layer'] = 1\n",
    "# In-memory input_fn does not use batching.\n",
    "def make_inmemory_train_input_fn(X, y):\n",
    "  y = np.expand_dims(y, axis=1)\n",
    "  def input_fn():\n",
    "    return dict(X), y\n",
    "  return input_fn\n",
    "train_input_fn = make_inmemory_train_input_fn(dftrain, y_train)\n",
    "\n",
    "# Train the model.\n",
    "est = tf.estimator.BoostedTreesClassifier(\n",
    "    feature_columns, \n",
    "    train_in_memory=True, \n",
    "    **in_memory_params)\n",
    "\n",
    "est.train(train_input_fn)\n",
    "print(est.evaluate(eval_input_fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns_colors = sns.color_palette('colorblind')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dicts = list(est.experimental_predict_with_explanations(eval_input_fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DFC Pandas dataframe.\n",
    "labels = y_eval.values\n",
    "probs = pd.Series([pred['probabilities'][1] for pred in pred_dicts])\n",
    "df_dfc = pd.DataFrame([pred['dfc'] for pred in pred_dicts])\n",
    "df_dfc.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum of DFCs + bias == probabality.\n",
    "bias = pred_dicts[0]['bias']\n",
    "dfc_prob = df_dfc.sum(axis=1) + bias\n",
    "np.testing.assert_almost_equal(dfc_prob.values,\n",
    "                               probs.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boilerplate code for plotting :)\n",
    "def _get_color(value):\n",
    "    \"\"\"To make positive DFCs plot green, negative DFCs plot red.\"\"\"\n",
    "    green, red = sns.color_palette()[2:4]\n",
    "    if value >= 0: return green\n",
    "    return red\n",
    "\n",
    "def _add_feature_values(feature_values, ax):\n",
    "    \"\"\"Display feature's values on left of plot.\"\"\"\n",
    "    x_coord = ax.get_xlim()[0]\n",
    "    OFFSET = 0.15\n",
    "    for y_coord, (feat_name, feat_val) in enumerate(feature_values.items()):\n",
    "        t = plt.text(x_coord, y_coord - OFFSET, '{}'.format(feat_val), size=12)\n",
    "        t.set_bbox(dict(facecolor='white', alpha=0.5))\n",
    "    from matplotlib.font_manager import FontProperties\n",
    "    font = FontProperties()\n",
    "    font.set_weight('bold')\n",
    "    t = plt.text(x_coord, y_coord + 1 - OFFSET, 'feature\\nvalue',\n",
    "    fontproperties=font, size=12)\n",
    "\n",
    "def plot_example(example):\n",
    "  TOP_N = 8 # View top 8 features.\n",
    "  sorted_ix = example.abs().sort_values()[-TOP_N:].index  # Sort by magnitude.\n",
    "  example = example[sorted_ix]\n",
    "  colors = example.map(_get_color).tolist()\n",
    "  ax = example.to_frame().plot(kind='barh',\n",
    "                          color=[colors],\n",
    "                          legend=None,\n",
    "                          alpha=0.75,\n",
    "                          figsize=(10,6))\n",
    "  ax.grid(False, axis='y')\n",
    "  ax.set_yticklabels(ax.get_yticklabels(), size=14)\n",
    "\n",
    "  # Add feature values.\n",
    "  _add_feature_values(dfeval.iloc[ID][sorted_ix], ax)\n",
    "  return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results.\n",
    "ID = 182\n",
    "example = df_dfc.iloc[ID]  # Choose ith example from evaluation set.\n",
    "TOP_N = 8  # View top 8 features.\n",
    "sorted_ix = example.abs().sort_values()[-TOP_N:].index\n",
    "ax = plot_example(example)\n",
    "ax.set_title('Feature contributions for example {}\\n pred: {:1.2f}; label: {}'.format(ID, probs[ID], labels[ID]))\n",
    "ax.set_xlabel('Contribution to predicted probability', size=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boilerplate plotting code.\n",
    "def dist_violin_plot(df_dfc, ID):\n",
    "  # Initialize plot.\n",
    "  fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "\n",
    "  # Create example dataframe.\n",
    "  TOP_N = 8  # View top 8 features.\n",
    "  example = df_dfc.iloc[ID]\n",
    "  ix = example.abs().sort_values()[-TOP_N:].index\n",
    "  example = example[ix]\n",
    "  example_df = example.to_frame(name='dfc')\n",
    "\n",
    "  # Add contributions of entire distribution.\n",
    "  parts=ax.violinplot([df_dfc[w] for w in ix],\n",
    "                 vert=False,\n",
    "                 showextrema=False,\n",
    "                 widths=0.7,\n",
    "                 positions=np.arange(len(ix)))\n",
    "  face_color = sns_colors[0]\n",
    "  alpha = 0.15\n",
    "  for pc in parts['bodies']:\n",
    "      pc.set_facecolor(face_color)\n",
    "      pc.set_alpha(alpha)\n",
    "\n",
    "  # Add feature values.\n",
    "  _add_feature_values(dfeval.iloc[ID][sorted_ix], ax)\n",
    "\n",
    "  # Add local contributions.\n",
    "  ax.scatter(example,\n",
    "              np.arange(example.shape[0]),\n",
    "              color=sns.color_palette()[2],\n",
    "              s=100,\n",
    "              marker=\"s\",\n",
    "              label='contributions for example')\n",
    "\n",
    "  # Legend\n",
    "  # Proxy plot, to show violinplot dist on legend.\n",
    "  ax.plot([0,0], [1,1], label='eval set contributions\\ndistributions',\n",
    "          color=face_color, alpha=alpha, linewidth=10)\n",
    "  legend = ax.legend(loc='lower right', shadow=True, fontsize='x-large',\n",
    "                     frameon=True)\n",
    "  legend.get_frame().set_facecolor('white')\n",
    "\n",
    "  # Format plot.\n",
    "  ax.set_yticks(np.arange(example.shape[0]))\n",
    "  ax.set_yticklabels(example.index)\n",
    "  ax.grid(False, axis='y')\n",
    "  ax.set_xlabel('Contribution to predicted probability', size=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_violin_plot(df_dfc, ID)\n",
    "plt.title('Feature contributions for example {}\\n pred: {:1.2f}; label: {}'.format(ID, probs[ID], labels[ID]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = est.experimental_feature_importances(normalize=True)\n",
    "df_imp = pd.Series(importances)\n",
    "\n",
    "# Visualize importances.\n",
    "N = 8\n",
    "ax = (df_imp.iloc[0:N][::-1]\n",
    "    .plot(kind='barh',\n",
    "          color=sns_colors[0],\n",
    "          title='Gain feature importances',\n",
    "          figsize=(10, 6)))\n",
    "ax.grid(False, axis='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot.\n",
    "dfc_mean = df_dfc.abs().mean()\n",
    "N = 8\n",
    "sorted_ix = dfc_mean.abs().sort_values()[-N:].index  # Average and sort by absolute.\n",
    "ax = dfc_mean[sorted_ix].plot(kind='barh',\n",
    "                       color=sns_colors[1],\n",
    "                       title='Mean |directional feature contributions|',\n",
    "                       figsize=(10, 6))\n",
    "ax.grid(False, axis='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE = 'fare'\n",
    "feature = pd.Series(df_dfc[FEATURE].values, index=dfeval[FEATURE].values).sort_index()\n",
    "ax = sns.regplot(feature.index.values, feature.values, lowess=True)\n",
    "ax.set_ylabel('contribution')\n",
    "ax.set_xlabel(FEATURE)\n",
    "ax.set_xlim(0, 100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def permutation_importances(est, X_eval, y_eval, metric, features):\n",
    "    \"\"\"Column by column, shuffle values and observe effect on eval set.\n",
    "\n",
    "    source: http://explained.ai/rf-importance/index.html\n",
    "    A similar approach can be done during training. See \"Drop-column importance\"\n",
    "    in the above article.\"\"\"\n",
    "    baseline = metric(est, X_eval, y_eval)\n",
    "    imp = []\n",
    "    for col in features:\n",
    "        save = X_eval[col].copy()\n",
    "        X_eval[col] = np.random.permutation(X_eval[col])\n",
    "        m = metric(est, X_eval, y_eval)\n",
    "        X_eval[col] = save\n",
    "        imp.append(baseline - m)\n",
    "    return np.array(imp)\n",
    "\n",
    "def accuracy_metric(est, X, y):\n",
    "    \"\"\"TensorFlow estimator accuracy.\"\"\"\n",
    "    eval_input_fn = make_input_fn(X,\n",
    "                                  y=y,\n",
    "                                  shuffle=False,\n",
    "                                  n_epochs=1)\n",
    "    return est.evaluate(input_fn=eval_input_fn)['accuracy']\n",
    "features = CATEGORICAL_COLUMNS + NUMERIC_COLUMNS\n",
    "importances = permutation_importances(est, dfeval, y_eval, accuracy_metric,\n",
    "                                      features)\n",
    "df_imp = pd.Series(importances, index=features)\n",
    "\n",
    "sorted_ix = df_imp.abs().sort_values().index\n",
    "ax = df_imp[sorted_ix][-5:].plot(kind='barh', color=sns_colors[2], figsize=(10, 6))\n",
    "ax.grid(False, axis='y')\n",
    "ax.set_title('Permutation feature importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import uniform, seed\n",
    "from matplotlib.mlab import griddata\n",
    "\n",
    "# Create fake data\n",
    "seed(0)\n",
    "npts = 5000\n",
    "x = uniform(-2, 2, npts)\n",
    "y = uniform(-2, 2, npts)\n",
    "z = x*np.exp(-x**2 - y**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep data for training.\n",
    "df = pd.DataFrame({'x': x, 'y': y, 'z': z})\n",
    "\n",
    "xi = np.linspace(-2.0, 2.0, 200),\n",
    "yi = np.linspace(-2.1, 2.1, 210),\n",
    "xi,yi = np.meshgrid(xi, yi)\n",
    "\n",
    "df_predict = pd.DataFrame({\n",
    "    'x' : xi.flatten(),\n",
    "    'y' : yi.flatten(),\n",
    "})\n",
    "predict_shape = xi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_contour(x, y, z, **kwargs):\n",
    "  # Grid the data.\n",
    "  plt.figure(figsize=(10, 8))\n",
    "  # Contour the gridded data, plotting dots at the nonuniform data points.\n",
    "  CS = plt.contour(x, y, z, 15, linewidths=0.5, colors='k')\n",
    "  CS = plt.contourf(x, y, z, 15,\n",
    "                    vmax=abs(zi).max(), vmin=-abs(zi).max(), cmap='RdBu_r')\n",
    "  plt.colorbar()  # Draw colorbar.\n",
    "  # Plot data points.\n",
    "  plt.xlim(-2, 2)\n",
    "  plt.ylim(-2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zi = griddata(x, y, z, xi, yi, interp='linear')\n",
    "plot_contour(xi, yi, zi)\n",
    "plt.scatter(df.x, df.y, marker='.')\n",
    "plt.title('Contour on training data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc = [tf.feature_column.numeric_column('x'),\n",
    "      tf.feature_column.numeric_column('y')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(est):\n",
    "  \"\"\"Predictions from a given estimator.\"\"\"\n",
    "  predict_input_fn = lambda: tf.data.Dataset.from_tensors(dict(df_predict))\n",
    "  preds = np.array([p['predictions'][0] for p in est.predict(predict_input_fn)])\n",
    "  return preds.reshape(predict_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_fn = make_input_fn(df, df.z)\n",
    "est = tf.estimator.LinearRegressor(fc)\n",
    "est.train(train_input_fn, max_steps=500);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_contour(xi, yi, predict(est))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trees = 22 #@param {type: \"slider\", min: 1, max: 80, step: 1}\n",
    "\n",
    "est = tf.estimator.BoostedTreesRegressor(fc, n_batches_per_layer=1, n_trees=n_trees)\n",
    "est.train(train_input_fn, max_steps=500)\n",
    "clear_output()\n",
    "plot_contour(xi, yi, predict(est))\n",
    "plt.text(-1.8, 2.1, '# trees: {}'.format(n_trees), color='w', backgroundcolor='black', size=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
